from typing import List
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# pylint: disable=no-name-in-module
from langchain import schema
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore
from dotenv import dotenv_values


config = dotenv_values(".env")

# app config
st.set_page_config(page_title="Streaming bot", page_icon="ğŸ¤–")
st.title("Streaming bot")


def get_response(
    user_query: str, chat_history: List[schema.HumanMessage], collection_name: str
):
    """
    Generates a response to the user's query based on the provided
    chat history and a specified document collection.
    Args:
        user_query: The query from the user.
        chat_history: The history of the chat as a list of HumanMessage objects.
        collection_name: The name of the document collection to use for reference.
    Returns:
        generator: A generator that streams the response to the user's query.
    Raises:
        ValueError: If any required configuration is missing or invalid.
    """
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½å°ˆé–€æ ¹æ“šæ–‡ä»¶å›ç­”å•é¡Œçš„ AI åŠ©æ‰‹ã€‚å¦‚æœä½ ç„¡æ³•å¾æ–‡ä»¶å¾—åˆ°ç­”æ¡ˆï¼Œè«‹èªªä½ ä¸çŸ¥é“ã€‚"
        "è«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œï¼š"
        "æ­·å²ç´€éŒ„ï¼š{chat_history}"
        "åƒè€ƒè³‡æ–™ï¼š{context}"
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )
    generator_llm = AzureChatOpenAI(
        azure_endpoint=config.get("AZURE_OPENAI_ENDPOINT"),
        azure_deployment=config.get("AZURE_OPENAI_DEPLOYMENT_NAME"),
        openai_api_version=config.get("AZURE_OPENAI_API_VERSION"),
        api_key=config.get("AZURE_OPENAI_KEY"),
        streaming=True,
    )
    embedding_llm = AzureOpenAIEmbeddings(
        azure_endpoint=config.get("AZURE_OPENAI_ENDPOINT"),
        azure_deployment=config.get("AZURE_OPENAI_Embedding_DEPLOYMENT_NAME"),
        api_key=config.get("AZURE_OPENAI_KEY"),
        openai_api_version=config.get("AZURE_OPENAI_API_VERSION"),
    )
    question_answer_chain = create_stuff_documents_chain(generator_llm, prompt)
    client = QdrantClient(url="http://localhost:6333")
    qdrant = QdrantVectorStore(
        client=client, collection_name=collection_name, embedding=embedding_llm
    )
    retriever = qdrant.as_retriever(search_kwargs={"k": 3})

    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    chain = rag_chain.pick("answer")
    return chain.stream({"input": user_query, "chat_history": chat_history})


def main():
    """
    main function for the Streamlit app.
    """

    # collection_name = st.sidebar.text_input(
    #     "è«‹è¼¸å…¥è¦æŸ¥è©¢çš„ Collection åç¨±", value="DefaultCollection"
    # )
    collections = ["labor_docs", "T-cross", "test"]
    collection_name = st.sidebar.selectbox(
        "è«‹é¸æ“‡è¦æŸ¥è©¢çš„ Collection åç¨±", options=collections
    )

    # session state
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = [
            AIMessage(content="Hello, I am a bot. How can I help you?"),
        ]

    # conversation
    for message in st.session_state.chat_history:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.write(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.write(message.content)

    # user input
    user_query = st.chat_input("Type your message here...")
    if user_query is not None and user_query != "":
        st.session_state.chat_history.append(HumanMessage(content=user_query))

        with st.chat_message("Human"):
            st.markdown(user_query)

        with st.chat_message("AI"):
            response = st.write_stream(
                get_response(
                    user_query=user_query,
                    chat_history=st.session_state.chat_history,
                    collection_name=collection_name,
                )
            )

        st.session_state.chat_history.append(AIMessage(content=response))


if __name__ == "__main__":
    main()
